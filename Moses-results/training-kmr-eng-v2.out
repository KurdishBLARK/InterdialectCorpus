nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/hossein/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Sat Aug 15 12:20:59 +03 2020
Executing: mkdir -p /home/hossein/working2/train/corpus
(1.0) selecting factors @ Sat Aug 15 12:20:59 +03 2020
(1.1) running mkcls  @ Sat Aug 15 12:20:59 +03 2020
/home/hossein/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.kmr -V/home/hossein/working2/train/corpus/kmr.vcb.classes opt
Executing: /home/hossein/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.kmr -V/home/hossein/working2/train/corpus/kmr.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 5969

start-costs: MEAN: 374214 (374084-374343)  SIGMA:129.167   
  end-costs: MEAN: 329322 (329272-329372)  SIGMA:49.9555   
   start-pp: MEAN: 572.459 (570.434-574.483)  SIGMA:2.0246   
     end-pp: MEAN: 167.466 (167.237-167.695)  SIGMA:0.229063   
 iterations: MEAN: 143342 (140383-146300)  SIGMA:2958.5   
       time: MEAN: 1.94318 (1.91258-1.97377)  SIGMA:0.030597   
(1.1) running mkcls  @ Sat Aug 15 12:21:03 +03 2020
/home/hossein/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.en -V/home/hossein/working2/train/corpus/en.vcb.classes opt
Executing: /home/hossein/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.en -V/home/hossein/working2/train/corpus/en.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 4896

start-costs: MEAN: 362864 (362638-363089)  SIGMA:225.509   
  end-costs: MEAN: 324043 (323919-324166)  SIGMA:123.82   
   start-pp: MEAN: 396.582 (394.067-399.097)  SIGMA:2.5148   
     end-pp: MEAN: 133.115 (132.652-133.579)  SIGMA:0.463481   
 iterations: MEAN: 120484 (118761-122207)  SIGMA:1723   
       time: MEAN: 1.73161 (1.71337-1.74984)  SIGMA:0.0182365   
(1.2) creating vcb file /home/hossein/working2/train/corpus/kmr.vcb @ Sat Aug 15 12:21:07 +03 2020
(1.2) creating vcb file /home/hossein/working2/train/corpus/en.vcb @ Sat Aug 15 12:21:07 +03 2020
(1.3) numberizing corpus /home/hossein/working2/train/corpus/kmr-en-int-train.snt @ Sat Aug 15 12:21:07 +03 2020
(1.3) numberizing corpus /home/hossein/working2/train/corpus/en-kmr-int-train.snt @ Sat Aug 15 12:21:07 +03 2020
(2) running giza @ Sat Aug 15 12:21:07 +03 2020
(2.1a) running snt2cooc kmr-en @ Sat Aug 15 12:21:07 +03 2020

Executing: mkdir -p /home/hossein/working2/train/giza.kmr-en
Executing: /home/hossein/mosesdecoder/tools/snt2cooc.out /home/hossein/working2/train/corpus/en.vcb /home/hossein/working2/train/corpus/kmr.vcb /home/hossein/working2/train/corpus/kmr-en-int-train.snt > /home/hossein/working2/train/giza.kmr-en/kmr-en.cooc
/home/hossein/mosesdecoder/tools/snt2cooc.out /home/hossein/working2/train/corpus/en.vcb /home/hossein/working2/train/corpus/kmr.vcb /home/hossein/working2/train/corpus/kmr-en-int-train.snt > /home/hossein/working2/train/giza.kmr-en/kmr-en.cooc
line 1000
END.
(2.1b) running giza kmr-en @ Sat Aug 15 12:21:07 +03 2020
/home/hossein/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hossein/working2/train/giza.kmr-en/kmr-en.cooc -c /home/hossein/working2/train/corpus/kmr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hossein/working2/train/giza.kmr-en/kmr-en -onlyaldumps 1 -p0 0.999 -s /home/hossein/working2/train/corpus/en.vcb -t /home/hossein/working2/train/corpus/kmr.vcb
Executing: /home/hossein/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hossein/working2/train/giza.kmr-en/kmr-en.cooc -c /home/hossein/working2/train/corpus/kmr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hossein/working2/train/giza.kmr-en/kmr-en -onlyaldumps 1 -p0 0.999 -s /home/hossein/working2/train/corpus/en.vcb -t /home/hossein/working2/train/corpus/kmr.vcb
/home/hossein/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hossein/working2/train/giza.kmr-en/kmr-en.cooc -c /home/hossein/working2/train/corpus/kmr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hossein/working2/train/giza.kmr-en/kmr-en -onlyaldumps 1 -p0 0.999 -s /home/hossein/working2/train/corpus/en.vcb -t /home/hossein/working2/train/corpus/kmr.vcb
Parameter 'coocurrencefile' changed from '' to '/home/hossein/working2/train/giza.kmr-en/kmr-en.cooc'
Parameter 'c' changed from '' to '/home/hossein/working2/train/corpus/kmr-en-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2020-08-15.122107.hossein' to '/home/hossein/working2/train/giza.kmr-en/kmr-en'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/hossein/working2/train/corpus/en.vcb'
Parameter 't' changed from '' to '/home/hossein/working2/train/corpus/kmr.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2020-08-15.122107.hossein.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hossein/working2/train/giza.kmr-en/kmr-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hossein/working2/train/corpus/kmr-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hossein/working2/train/corpus/en.vcb  (source vocabulary file name)
t = /home/hossein/working2/train/corpus/kmr.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2020-08-15.122107.hossein.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hossein/working2/train/giza.kmr-en/kmr-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hossein/working2/train/corpus/kmr-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hossein/working2/train/corpus/en.vcb  (source vocabulary file name)
t = /home/hossein/working2/train/corpus/kmr.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/hossein/working2/train/corpus/en.vcb
Reading vocabulary file from:/home/hossein/working2/train/corpus/kmr.vcb
Source vocabulary list has 4897 unique tokens 
Target vocabulary list has 5970 unique tokens 
Calculating vocabulary frequencies from corpus /home/hossein/working2/train/corpus/kmr-en-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 1608 sentence pairs.
 Train total # sentence pairs (weighted): 1608
Size of source portion of the training corpus: 33954 tokens
Size of the target portion of the training corpus: 34914 tokens 
In source portion of the training corpus, only 4896 unique tokens appeared
In target portion of the training corpus, only 5968 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 34914/(35562-1608)== 1.02827
There are 378082 378082 entries in table
==========================================================
Model1 Training Started at: Sat Aug 15 12:21:07 2020

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 12.7478 PERPLEXITY 6878.16
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 17.3799 PERPLEXITY 170560
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 6.66436 PERPLEXITY 101.431
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 9.3531 PERPLEXITY 653.98
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 6.20518 PERPLEXITY 73.7813
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.58182 PERPLEXITY 383.164
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 5.99052 PERPLEXITY 63.581
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 8.03622 PERPLEXITY 262.508
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 5.8787 PERPLEXITY 58.8391
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.6963 PERPLEXITY 207.404
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 4896  #classes: 51
Read classes: #words: 5969  #classes: 51

==========================================================
Hmm Training Started at: Sat Aug 15 12:21:08 2020

-----------
Hmm: Iteration 1
A/D table contains 119074 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 5.77968 PERPLEXITY 54.9362
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.47619 PERPLEXITY 178.057

Hmm Iteration: 1 took: 1 seconds

-----------
Hmm: Iteration 2
A/D table contains 119074 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 5.69833 PERPLEXITY 51.924
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 6.64526 PERPLEXITY 100.097

Hmm Iteration: 2 took: 1 seconds

-----------
Hmm: Iteration 3
A/D table contains 119074 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 5.30479 PERPLEXITY 39.5277
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 5.89889 PERPLEXITY 59.6682

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
A/D table contains 119074 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 4.9481 PERPLEXITY 30.8692
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 5.36823 PERPLEXITY 41.3045

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 119074 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.70238 PERPLEXITY 26.0349
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 5.02897 PERPLEXITY 32.649

Hmm Iteration: 5 took: 1 seconds

Entire Hmm Training took: 4 seconds
==========================================================
Read classes: #words: 4896  #classes: 51
Read classes: #words: 5969  #classes: 51
Read classes: #words: 4896  #classes: 51
Read classes: #words: 5969  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sat Aug 15 12:21:12 2020


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 840.396 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 119074 parameters.
A/D table contains 118056 parameters.
NTable contains 48970 parameter.
p0_count is 27647.5 and p1 is 3633.25; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 4.64478 PERPLEXITY 25.016
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 4.77658 PERPLEXITY 27.409

THTo3 Viterbi Iteration : 1 took: 1 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 842.662 #alsophisticatedcountcollection: 0 #hcsteps: 3.45274
#peggingImprovements: 0
A/D table contains 119074 parameters.
A/D table contains 117827 parameters.
NTable contains 48970 parameter.
p0_count is 32030.7 and p1 is 1441.67; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 5.68967 PERPLEXITY 51.6132
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 5.7797 PERPLEXITY 54.9369

Model3 Viterbi Iteration : 2 took: 1 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 842.813 #alsophisticatedcountcollection: 0 #hcsteps: 3.72015
#peggingImprovements: 0
A/D table contains 119074 parameters.
A/D table contains 117543 parameters.
NTable contains 48970 parameter.
p0_count is 32896.6 and p1 is 1008.69; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 5.48289 PERPLEXITY 44.7214
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 5.55562 PERPLEXITY 47.0335

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 842.884 #alsophisticatedcountcollection: 41.4142 #hcsteps: 3.81343
#peggingImprovements: 0
D4 table contains 517041 parameters.
A/D table contains 119074 parameters.
A/D table contains 117286 parameters.
NTable contains 48970 parameter.
p0_count is 33219 and p1 is 847.514; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 5.42114 PERPLEXITY 42.8475
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 5.48661 PERPLEXITY 44.8368

T3To4 Viterbi Iteration : 4 took: 1 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 842.807 #alsophisticatedcountcollection: 41.1331 #hcsteps: 3.42164
#peggingImprovements: 0
D4 table contains 517041 parameters.
A/D table contains 119074 parameters.
A/D table contains 117563 parameters.
NTable contains 48970 parameter.
p0_count is 32829.2 and p1 is 1042.42; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 5.3562 PERPLEXITY 40.9615
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 5.40686 PERPLEXITY 42.4256

Model4 Viterbi Iteration : 5 took: 2 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 842.762 #alsophisticatedcountcollection: 31.8694 #hcsteps: 3.28918
#peggingImprovements: 0
D4 table contains 517041 parameters.
A/D table contains 119074 parameters.
A/D table contains 116747 parameters.
NTable contains 48970 parameter.
p0_count is 32793.7 and p1 is 1060.15; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 5.1836 PERPLEXITY 36.3428
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 5.22208 PERPLEXITY 37.3252

Model4 Viterbi Iteration : 6 took: 2 seconds
H333444 Training Finished at: Sat Aug 15 12:21:19 2020


Entire Viterbi H333444 Training took: 7 seconds
==========================================================

Entire Training took: 12 seconds
Program Finished at: Sat Aug 15 12:21:19 2020

==========================================================
Executing: rm -f /home/hossein/working2/train/giza.kmr-en/kmr-en.A3.final.gz
Executing: gzip /home/hossein/working2/train/giza.kmr-en/kmr-en.A3.final
(2.1a) running snt2cooc en-kmr @ Sat Aug 15 12:21:19 +03 2020

Executing: mkdir -p /home/hossein/working2/train/giza.en-kmr
Executing: /home/hossein/mosesdecoder/tools/snt2cooc.out /home/hossein/working2/train/corpus/kmr.vcb /home/hossein/working2/train/corpus/en.vcb /home/hossein/working2/train/corpus/en-kmr-int-train.snt > /home/hossein/working2/train/giza.en-kmr/en-kmr.cooc
/home/hossein/mosesdecoder/tools/snt2cooc.out /home/hossein/working2/train/corpus/kmr.vcb /home/hossein/working2/train/corpus/en.vcb /home/hossein/working2/train/corpus/en-kmr-int-train.snt > /home/hossein/working2/train/giza.en-kmr/en-kmr.cooc
line 1000
END.
(2.1b) running giza en-kmr @ Sat Aug 15 12:21:19 +03 2020
/home/hossein/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hossein/working2/train/giza.en-kmr/en-kmr.cooc -c /home/hossein/working2/train/corpus/en-kmr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hossein/working2/train/giza.en-kmr/en-kmr -onlyaldumps 1 -p0 0.999 -s /home/hossein/working2/train/corpus/kmr.vcb -t /home/hossein/working2/train/corpus/en.vcb
Executing: /home/hossein/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hossein/working2/train/giza.en-kmr/en-kmr.cooc -c /home/hossein/working2/train/corpus/en-kmr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hossein/working2/train/giza.en-kmr/en-kmr -onlyaldumps 1 -p0 0.999 -s /home/hossein/working2/train/corpus/kmr.vcb -t /home/hossein/working2/train/corpus/en.vcb
/home/hossein/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hossein/working2/train/giza.en-kmr/en-kmr.cooc -c /home/hossein/working2/train/corpus/en-kmr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hossein/working2/train/giza.en-kmr/en-kmr -onlyaldumps 1 -p0 0.999 -s /home/hossein/working2/train/corpus/kmr.vcb -t /home/hossein/working2/train/corpus/en.vcb
Parameter 'coocurrencefile' changed from '' to '/home/hossein/working2/train/giza.en-kmr/en-kmr.cooc'
Parameter 'c' changed from '' to '/home/hossein/working2/train/corpus/en-kmr-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2020-08-15.122119.hossein' to '/home/hossein/working2/train/giza.en-kmr/en-kmr'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/hossein/working2/train/corpus/kmr.vcb'
Parameter 't' changed from '' to '/home/hossein/working2/train/corpus/en.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2020-08-15.122119.hossein.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hossein/working2/train/giza.en-kmr/en-kmr  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hossein/working2/train/corpus/en-kmr-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hossein/working2/train/corpus/kmr.vcb  (source vocabulary file name)
t = /home/hossein/working2/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2020-08-15.122119.hossein.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hossein/working2/train/giza.en-kmr/en-kmr  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hossein/working2/train/corpus/en-kmr-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hossein/working2/train/corpus/kmr.vcb  (source vocabulary file name)
t = /home/hossein/working2/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/hossein/working2/train/corpus/kmr.vcb
Reading vocabulary file from:/home/hossein/working2/train/corpus/en.vcb
Source vocabulary list has 5970 unique tokens 
Target vocabulary list has 4897 unique tokens 
Calculating vocabulary frequencies from corpus /home/hossein/working2/train/corpus/en-kmr-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 1608 sentence pairs.
 Train total # sentence pairs (weighted): 1608
Size of source portion of the training corpus: 34914 tokens
Size of the target portion of the training corpus: 33954 tokens 
In source portion of the training corpus, only 5969 unique tokens appeared
In target portion of the training corpus, only 4895 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 33954/(36522-1608)== 0.972504
There are 377009 377009 entries in table
==========================================================
Model1 Training Started at: Sat Aug 15 12:21:19 2020

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 12.4759 PERPLEXITY 5696.47
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 17.1319 PERPLEXITY 143618
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 6.26042 PERPLEXITY 76.661
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 9.15869 PERPLEXITY 571.53
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.81077 PERPLEXITY 56.1328
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.35292 PERPLEXITY 326.949
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 5.60077 PERPLEXITY 48.5289
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 7.79353 PERPLEXITY 221.863
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 5.49071 PERPLEXITY 44.9644
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.44751 PERPLEXITY 174.552
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 5969  #classes: 51
Read classes: #words: 4896  #classes: 51

==========================================================
Hmm Training Started at: Sat Aug 15 12:21:20 2020

-----------
Hmm: Iteration 1
A/D table contains 120995 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 5.3979 PERPLEXITY 42.1629
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.22475 PERPLEXITY 149.578

Hmm Iteration: 1 took: 1 seconds

-----------
Hmm: Iteration 2
A/D table contains 120995 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 5.35177 PERPLEXITY 40.8361
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 6.3712 PERPLEXITY 82.7796

Hmm Iteration: 2 took: 1 seconds

-----------
Hmm: Iteration 3
A/D table contains 120995 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 5.01516 PERPLEXITY 32.338
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 5.68689 PERPLEXITY 51.5139

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
A/D table contains 120995 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 4.70023 PERPLEXITY 25.9962
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 5.18247 PERPLEXITY 36.3144

Hmm Iteration: 4 took: 1 seconds

-----------
Hmm: Iteration 5
A/D table contains 120995 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.47116 PERPLEXITY 22.1795
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 4.84387 PERPLEXITY 28.7178

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 4 seconds
==========================================================
Read classes: #words: 5969  #classes: 51
Read classes: #words: 4896  #classes: 51
Read classes: #words: 5969  #classes: 51
Read classes: #words: 4896  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sat Aug 15 12:21:25 2020


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 826.331 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 120995 parameters.
A/D table contains 117075 parameters.
NTable contains 59700 parameter.
p0_count is 24622.7 and p1 is 4665.66; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 4.33907 PERPLEXITY 20.2391
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 4.45987 PERPLEXITY 22.0066

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 831.147 #alsophisticatedcountcollection: 0 #hcsteps: 3.91667
#peggingImprovements: 0
A/D table contains 120995 parameters.
A/D table contains 116585 parameters.
NTable contains 59700 parameter.
p0_count is 30574.9 and p1 is 1689.56; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 5.53571 PERPLEXITY 46.3891
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 5.64294 PERPLEXITY 49.9683

Model3 Viterbi Iteration : 2 took: 1 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 831.499 #alsophisticatedcountcollection: 0 #hcsteps: 4.27799
#peggingImprovements: 0
A/D table contains 120995 parameters.
A/D table contains 116380 parameters.
NTable contains 59700 parameter.
p0_count is 31922.2 and p1 is 1015.89; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 5.25798 PERPLEXITY 38.2657
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 5.34405 PERPLEXITY 40.6179

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 831.624 #alsophisticatedcountcollection: 48.4328 #hcsteps: 4.39801
#peggingImprovements: 0
D4 table contains 517041 parameters.
A/D table contains 120995 parameters.
A/D table contains 116318 parameters.
NTable contains 59700 parameter.
p0_count is 32307.1 and p1 is 823.467; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 5.17841 PERPLEXITY 36.2124
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 5.25561 PERPLEXITY 38.2028

T3To4 Viterbi Iteration : 4 took: 1 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 831.646 #alsophisticatedcountcollection: 51.0311 #hcsteps: 3.9291
#peggingImprovements: 0
D4 table contains 517041 parameters.
A/D table contains 120995 parameters.
A/D table contains 116113 parameters.
NTable contains 59700 parameter.
p0_count is 31892.2 and p1 is 1030.89; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 5.30538 PERPLEXITY 39.5438
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 5.36981 PERPLEXITY 41.35

Model4 Viterbi Iteration : 5 took: 2 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 831.631 #alsophisticatedcountcollection: 40.5529 #hcsteps: 3.77861
#peggingImprovements: 0
D4 table contains 517041 parameters.
A/D table contains 120995 parameters.
A/D table contains 114952 parameters.
NTable contains 59700 parameter.
p0_count is 31902.4 and p1 is 1025.78; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 5.11595 PERPLEXITY 34.6779
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 5.16588 PERPLEXITY 35.8993

Model4 Viterbi Iteration : 6 took: 3 seconds
H333444 Training Finished at: Sat Aug 15 12:21:32 2020


Entire Viterbi H333444 Training took: 7 seconds
==========================================================

Entire Training took: 13 seconds
Program Finished at: Sat Aug 15 12:21:32 2020

==========================================================
Executing: rm -f /home/hossein/working2/train/giza.en-kmr/en-kmr.A3.final.gz
Executing: gzip /home/hossein/working2/train/giza.en-kmr/en-kmr.A3.final
(3) generate word alignment @ Sat Aug 15 12:21:32 +03 2020
Combining forward and inverted alignment from files:
  /home/hossein/working2/train/giza.kmr-en/kmr-en.A3.final.{bz2,gz}
  /home/hossein/working2/train/giza.en-kmr/en-kmr.A3.final.{bz2,gz}
Executing: mkdir -p /home/hossein/working2/train/model
Executing: /home/hossein/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/hossein/working2/train/giza.en-kmr/en-kmr.A3.final.gz" -i "gzip -cd /home/hossein/working2/train/giza.kmr-en/kmr-en.A3.final.gz" |/home/hossein/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/hossein/working2/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<1608>
(4) generate lexical translation table 0-0 @ Sat Aug 15 12:21:32 +03 2020
(/home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.kmr,/home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.en,/home/hossein/working2/train/model/lex)
!!
Saved: /home/hossein/working2/train/model/lex.f2e and /home/hossein/working2/train/model/lex.e2f
FILE: /home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.en
FILE: /home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.kmr
FILE: /home/hossein/working2/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Sat Aug 15 12:21:32 +03 2020
/home/hossein/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/hossein/mosesdecoder/scripts/../bin/extract /home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.en /home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.kmr /home/hossein/working2/train/model/aligned.grow-diag-final-and /home/hossein/working2/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/hossein/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/hossein/mosesdecoder/scripts/../bin/extract /home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.en /home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.kmr /home/hossein/working2/train/model/aligned.grow-diag-final-and /home/hossein/working2/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Sat Aug 15 12:21:32 2020
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/hossein/working2/train/model/tmp.10013; ls -l /home/hossein/working2/train/model/tmp.10013 
total=1608 line-per-split=403 
split -d -l 403 -a 7 /home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.en /home/hossein/working2/train/model/tmp.10013/target.split -d -l 403 -a 7 /home/hossein/corpus2/kmr-eng-v2.kmr-en.clean.kmr /home/hossein/working2/train/model/tmp.10013/source.split -d -l 403 -a 7 /home/hossein/working2/train/model/aligned.grow-diag-final-and /home/hossein/working2/train/model/tmp.10013/align.merging extract / extract.inv
gunzip -c /home/hossein/working2/train/model/tmp.10013/extract.0000000.gz /home/hossein/working2/train/model/tmp.10013/extract.0000001.gz /home/hossein/working2/train/model/tmp.10013/extract.0000002.gz /home/hossein/working2/train/model/tmp.10013/extract.0000003.gz  | LC_ALL=C sort     -T /home/hossein/working2/train/model/tmp.10013 2>> /dev/stderr | gzip -c > /home/hossein/working2/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/hossein/working2/train/model/tmp.10013/extract.0000000.inv.gz /home/hossein/working2/train/model/tmp.10013/extract.0000001.inv.gz /home/hossein/working2/train/model/tmp.10013/extract.0000002.inv.gz /home/hossein/working2/train/model/tmp.10013/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/hossein/working2/train/model/tmp.10013 2>> /dev/stderr | gzip -c > /home/hossein/working2/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/hossein/working2/train/model/tmp.10013/extract.0000000.o.gz /home/hossein/working2/train/model/tmp.10013/extract.0000001.o.gz /home/hossein/working2/train/model/tmp.10013/extract.0000002.o.gz /home/hossein/working2/train/model/tmp.10013/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/hossein/working2/train/model/tmp.10013 2>> /dev/stderr | gzip -c > /home/hossein/working2/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Sat Aug 15 12:21:33 2020
(6) score phrases @ Sat Aug 15 12:21:33 +03 2020
(6.1)  creating table half /home/hossein/working2/train/model/phrase-table.half.f2e @ Sat Aug 15 12:21:33 +03 2020
/home/hossein/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hossein/mosesdecoder/scripts/../bin/score /home/hossein/working2/train/model/extract.sorted.gz /home/hossein/working2/train/model/lex.f2e /home/hossein/working2/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/hossein/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hossein/mosesdecoder/scripts/../bin/score /home/hossein/working2/train/model/extract.sorted.gz /home/hossein/working2/train/model/lex.f2e /home/hossein/working2/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Sat Aug 15 12:21:33 2020
/home/hossein/mosesdecoder/scripts/../bin/score /home/hossein/working2/train/model/tmp.10062/extract.0.gz /home/hossein/working2/train/model/lex.f2e /home/hossein/working2/train/model/tmp.10062/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/hossein/working2/train/model/tmp.10062/run.0.sh/home/hossein/working2/train/model/tmp.10062/run.1.sh/home/hossein/working2/train/model/tmp.10062/run.2.sh/home/hossein/working2/train/model/tmp.10062/run.3.shmv /home/hossein/working2/train/model/tmp.10062/phrase-table.half.0000000.gz /home/hossein/working2/train/model/phrase-table.half.f2e.gzrm -rf /home/hossein/working2/train/model/tmp.10062 
Finished Sat Aug 15 12:21:35 2020
(6.3)  creating table half /home/hossein/working2/train/model/phrase-table.half.e2f @ Sat Aug 15 12:21:35 +03 2020
/home/hossein/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hossein/mosesdecoder/scripts/../bin/score /home/hossein/working2/train/model/extract.inv.sorted.gz /home/hossein/working2/train/model/lex.e2f /home/hossein/working2/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/hossein/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hossein/mosesdecoder/scripts/../bin/score /home/hossein/working2/train/model/extract.inv.sorted.gz /home/hossein/working2/train/model/lex.e2f /home/hossein/working2/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Sat Aug 15 12:21:35 2020
/home/hossein/mosesdecoder/scripts/../bin/score /home/hossein/working2/train/model/tmp.10092/extract.0.gz /home/hossein/working2/train/model/lex.e2f /home/hossein/working2/train/model/tmp.10092/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/hossein/working2/train/model/tmp.10092/run.0.sh/home/hossein/working2/train/model/tmp.10092/run.1.sh/home/hossein/working2/train/model/tmp.10092/run.2.sh/home/hossein/working2/train/model/tmp.10092/run.3.shgunzip -c /home/hossein/working2/train/model/tmp.10092/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/hossein/working2/train/model/tmp.10092  | gzip -c > /home/hossein/working2/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/hossein/working2/train/model/tmp.10092 
Finished Sat Aug 15 12:21:36 2020
(6.6) consolidating the two halves @ Sat Aug 15 12:21:36 +03 2020
Executing: /home/hossein/mosesdecoder/scripts/../bin/consolidate /home/hossein/working2/train/model/phrase-table.half.f2e.gz /home/hossein/working2/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/hossein/working2/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
.
Executing: rm -f /home/hossein/working2/train/model/phrase-table.half.*
(7) learn reordering model @ Sat Aug 15 12:21:37 +03 2020
(7.1) [no factors] learn reordering model @ Sat Aug 15 12:21:37 +03 2020
(7.2) building tables @ Sat Aug 15 12:21:37 +03 2020
Executing: /home/hossein/mosesdecoder/scripts/../bin/lexical-reordering-score /home/hossein/working2/train/model/extract.o.sorted.gz 0.5 /home/hossein/working2/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Sat Aug 15 12:21:37 +03 2020
  no generation model requested, skipping step
(9) create moses.ini @ Sat Aug 15 12:21:37 +03 2020
